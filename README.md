# RAG-chatbot-Gemini-API-ollama

# ---------------------------
# âœ… 1) Clone your repo
# ---------------------------
echo "ğŸ“¥ Cloning repo..."
git clone https://github.com/BhagyeshPatil2004/RAG-chatbot-Gemini-API-ollama.git
cd RAG-chatbot-Gemini-API-ollama

# ---------------------------
# âœ… 2) Create .env placeholder
# ---------------------------
echo "ğŸ”‘ Creating .env..."
echo 'GEMINI_API_KEY="YOUR_GEMINI_API_KEY"' > .env

# ---------------------------
# âœ… 3) Create .gitignore
# ---------------------------
echo "ğŸ—‚ï¸  Creating .gitignore..."
cat <<EOL > .gitignore
# Python
__pycache__/
*.pyc
.venv/

# Env
.env

# VSCode
.vscode/

# Chroma DB
chroma_db/
EOL

# ---------------------------
# âœ… 4) Write README.md
# ---------------------------
echo "ğŸ“ Writing README.md..."
cat <<EOL > README.md
# RAG-chatbot-Gemini-API-ollama ğŸ¤–

This is a simple Retrieval-Augmented Generation (RAG) chatbot built using:
- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://github.com/ollama/ollama)
- [ChromaDB](https://www.trychroma.com/)

It uses the **original codebasics FAQ dataset** from [this video](https://www.youtube.com/watch?v=AjQPRomyd-k).  
**Note:** I did **some changes in the code only â€” the dataset remains the same.**

---

## âš™ï¸ Features

âœ… Loads CSV data of FAQs  
âœ… Creates embeddings with Ollama (\`mxbai-embed-large\`)  
âœ… Stores embeddings locally in ChromaDB  
âœ… Uses Gemini API (or your local LLM) to generate answers  
âœ… Runs a simple CLI chat loop

---

## ğŸ“š Dataset

- \`codebasics_faqs.csv\` is sourced from the [codebasics YouTube channel](https://www.youtube.com/watch?v=AjQPRomyd-k)
- **No modifications to dataset** â€” only code improved to fit this RAG structure.

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Clone the Repo

\`\`\`bash
git clone https://github.com/BhagyeshPatil2004/RAG-chatbot-Gemini-API-ollama.git
cd RAG-chatbot-Gemini-API-ollama
\`\`\`

### 2ï¸âƒ£ Setup Python Environment

\`\`\`bash
python -m venv .venv
# Windows
.venv\\Scripts\\activate
# Linux/Mac
source .venv/bin/activate

pip install -r requirements.txt
\`\`\`

### 3ï¸âƒ£ Add your Gemini API key

Edit \`.env\` and replace with your real API key.

### 4ï¸âƒ£ Make sure Ollama is running

\`\`\`bash
ollama pull mxbai-embed-large
\`\`\`

### 5ï¸âƒ£ Run the chatbot

\`\`\`bash
python "AI Agent 1/agent.py"
\`\`\`

---

## âœ… Notes

- Ollama must be running for embeddings.
- Gemini API generates answers.
- Easily swap a bigger dataset for your major project.

---

## ğŸ‘¨â€ğŸ’» Author

**[Bhagyesh Patil](https://github.com/BhagyeshPatil2004)**  
ğŸ“Œ Repo: [RAG-chatbot-Gemini-API-ollama](https://github.com/BhagyeshPatil2004/RAG-chatbot-Gemini-API-ollama)

---
EOL

# ---------------------------
# âœ… 5) Create requirements.txt
# ---------------------------
echo "ğŸ“¦ Creating requirements.txt..."
cat <<EOL > requirements.txt
langchain
langchain_community
langchain_ollama
langchain_chroma
ollama
python-dotenv
EOL

# ---------------------------
# âœ… 6) Setup Python venv + install
# ---------------------------
echo "ğŸ Creating Python venv..."
python -m venv .venv

echo "ğŸ Activating venv & installing..."
# Detect OS to activate venv properly
if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "win32" ]]; then
    .venv\\Scripts\\activate && pip install -r requirements.txt
else
    source .venv/bin/activate && pip install -r requirements.txt
fi

# ---------------------------
# âœ… 7) Pull Ollama embedding model
# ---------------------------
echo "ğŸ§  Pulling Ollama embedding model..."
ollama pull mxbai-embed-large

# ---------------------------
# âœ… Done!
# ---------------------------
echo ""
echo "1ï¸âƒ£  Add your real Gemini API key in .env"
echo "2ï¸âƒ£  Make sure Ollama is running"
echo "3ï¸âƒ£  Run your bot: python \"AI Agent 1/agent.py\""
echo ""
echo "ğŸ”¥ Happy!"
